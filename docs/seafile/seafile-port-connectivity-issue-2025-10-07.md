# Seafile Port Connectivity Issue - October 7, 2025

## Executive Summary

**Incident Type**: Recurring port connectivity issue (NOT a crash)
**Date**: October 7, 2025 20:00-22:10 GMT+2
**Duration**: ~2 hours investigation and resolution
**Impact**: Seafile web interface inaccessible from external hosts
**Status**: ✅ **RESOLVED** - Port changed from 8080 to 8090

## Critical Finding: No Crash Occurred

### What Actually Happened
- **Seafile container**: Running continuously for 3 days (since Oct 4 16:20)
- **Container status**: Healthy, no crashes, no OOM kills
- **Exit code**: 0 (never exited)
- **Services**: All internal services (nginx, seahub, gunicorn) running normally
- **Issue**: Port 8080 became inaccessible from external hosts only

### Misconception
The user believed Seafile "crashed" but diagnostic evidence shows:
- Container never stopped or restarted unexpectedly
- All internal health checks passing
- Application responding correctly on localhost
- **Only symptom**: External connectivity to port 8080 failed

## Root Cause Analysis

### Primary Issue: Recurring Port-Specific Connectivity Failure

#### Evidence Trail
1. **Sept 27, 2025**: Port 50080 became inaccessible → fixed by changing to 8080
2. **Oct 4, 2025 13:46**: Network restart event in PCT 103
3. **Oct 7, 2025 20:00**: Port 8080 became inaccessible → fixed by changing to 8090

#### Connectivity Pattern
```
Local Access (within PCT 103):     ✅ Working
From my machine (192.168.1.134):   ✅ Working
From Proxmox host (pve2):          ❌ FAILED
From NPM host (192.168.1.9):       ❌ FAILED
```

#### Port Testing Results
| Port  | Date Tested | Local | External | Status |
|-------|-------------|-------|----------|--------|
| 50080 | Sept 27     | ✅    | ❌       | Abandoned |
| 8080  | Sept 27-Oct 7 | ✅  | ✅ → ❌  | Failed after time |
| 8090  | Oct 7       | ✅    | ✅       | Currently working |

### Technical Investigation Findings

#### 1. Network Event Timeline (Oct 4, 2025)
```
13:46:46 - Stopped network-online.target
13:46:46 - Stopped network.target
13:46:46 - Stopping networking.service
13:46:46 - Stopped networking.service
13:46:50 - Starting networking.service
13:46:50 - Finished networking.service
13:46:51 - Network services restarted
```

**Impact**: Network restart may have corrupted port bindings or iptables NAT rules

#### 2. Docker Port Binding Evidence
```bash
# Port was correctly mapped in Docker
docker port seafile
# Output: 80/tcp -> 0.0.0.0:8080

# Docker proxy was listening
ss -tlnp | grep 8080
# Output: LISTEN 0.0.0.0:8080 users:(("docker-proxy",pid=492))
```

**Finding**: Docker configuration was correct, but external routing failed

#### 3. iptables Analysis
```bash
# INPUT chain was empty (no blocking rules)
Chain INPUT (policy ACCEPT)
num   pkts bytes target     prot opt in     out     source               destination

# Previously had logging rules that were removed
1        6   399 LOG        tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8080
```

**Finding**: Someone had added iptables logging rules (now removed), suggesting previous debugging

#### 4. Network Connectivity Tests
```bash
# ICMP (ping) worked fine
192.168.1.9 → 192.168.1.25: ✅ 0.268ms latency

# TCP to port 8080 failed
curl http://192.168.1.25:8080: ❌ "Couldn't connect to server"

# But localhost worked
curl http://localhost (inside container): ✅ HTTP 302
```

**Finding**: TCP port forwarding specifically broken, not general network failure

## Why Port Changes Fix The Issue

### Hypothesis: Stale NAT/Connection Tracking State

When changing ports, the following happens:
1. **Docker recreates container** with new port mapping
2. **New iptables rules** generated by Docker (DNAT, FORWARD chain)
3. **Connection tracking flushed** for that specific port
4. **New docker-proxy process** starts with clean state

### Why Original Port Stops Working
Possible causes:
1. **Conntrack table corruption**: Old connection tracking entries persist
2. **iptables NAT rule stale**: DNAT rules not properly updated after network restart
3. **Docker bridge issue**: Network namespace routing corruption
4. **Selective port blocking**: Some external mechanism (firewall appliance?) blocking specific ports

## Diagnostic Evidence

### Container Never Crashed
```json
{
    "Status": "running",
    "Running": true,
    "OOMKilled": false,
    "Dead": false,
    "ExitCode": 0,
    "StartedAt": "2025-10-04T16:20:23Z",
    "FinishedAt": "0001-01-01T00:00:00Z"
}
```

### Service Health Monitoring
```bash
# Seafile health checks continuously passing
"Health": {
    "Status": "healthy",
    "FailingStreak": 0
}

# All services running normally
nginx:       ✅ Master + 16 workers
seahub:      ✅ 6 gunicorn workers
seaf-server: ✅ Running on port 8082
mysql:       ✅ Healthy
memcached:   ✅ Running
```

### Controller Log Timeline
```
2025-10-04 16:20:23 - Seafile started (last successful start)
2025-10-07 20:04:06 - Seafile shutdown (our first restart attempt)
2025-10-07 20:04:07 - Seafile started (restart for debugging)
2025-10-07 20:05:19 - Seafile shutdown (PCT reboot)
2025-10-07 20:05:29 - Seafile started (after PCT reboot)
2025-10-07 20:07:40 - Seafile shutdown (port change to 8090)
2025-10-07 20:07:42 - Seafile started (running on port 8090)
```

**Key Finding**: 3+ days uptime (Oct 4-7) with zero crashes

## Resolution Steps Taken

### Step 1: Verified Services Running (20:02)
- ✅ All containers healthy
- ✅ 6 gunicorn workers running
- ✅ Nginx processing requests
- ✅ Local connectivity working

### Step 2: Diagnosed Port Connectivity (20:02-20:04)
- ❌ Port 8080 failed from external hosts
- ✅ Port 8080 worked from my machine (192.168.1.134)
- ❌ Port 8080 failed from Proxmox host
- ❌ Port 8080 failed from NPM host

### Step 3: Attempted Container Restart (20:04)
- Restarted Seafile container
- **Result**: Port issue persisted

### Step 4: Attempted PCT Reboot (20:05)
- Rebooted entire PCT 103 container
- **Result**: Port issue persisted
- **Finding**: Problem not related to container/service state

### Step 5: Changed Port to 8090 (20:07)
```bash
# Updated docker-compose.yml
sed -i "s/8080:80/8090:80/" docker-compose.yml

# Recreated container
docker compose up -d seafile

# Updated NPM proxy configuration
sed -i "s/\$port.*8080/\$port           8090/" /data/nginx/proxy_host/17.conf
nginx -s reload
```

**Result**: ✅ Immediate connectivity restored from all hosts

## Current Working Configuration

### Seafile Container (PCT 103)
```yaml
# /opt/seafile-docker/docker-compose.yml
services:
  seafile:
    ports:
      - "8090:80"  # Changed from 8080
```

### Nginx Proxy Manager
```nginx
# /data/nginx/proxy_host/17.conf
set $port           8090;  # Changed from 8080
proxy_pass http://192.168.1.25:$port;
```

### Access Status
- ✅ Local: http://192.168.1.25:8090 → HTTP 302
- ✅ HTTPS: https://files.accelior.com → HTTP 302
- ✅ All hosts can connect to port 8090

## Unsolved Mystery: Why Do Ports Fail?

### What We Know
1. **Port binding works initially** (50080, 8080 both worked when first configured)
2. **Ports fail after time** (weeks to months of uptime)
3. **Network events trigger issues** (Oct 4 network restart preceded Oct 7 failure)
4. **Changing ports fixes immediately** (no other intervention needed)
5. **Not Docker-specific** (affects Docker proxy, iptables, entire network path)

### What We Don't Know
1. **Why specific ports fail** (50080, 8080) but others work (3306, 8090)
2. **What corrupts the port** (connection tracking? NAT table? external firewall?)
3. **Why my machine works** (192.168.1.134) but Proxmox/NPM hosts fail
4. **Exact trigger mechanism** (time-based? event-based? connection count?)

### Theories to Investigate
1. **Conntrack table limits**: Check `nf_conntrack_max` and current usage
2. **OPNsense firewall state**: Check if OPNsense has stale state for old ports
3. **Docker network plugin bug**: Investigate Docker bridge driver issues
4. **Proxmox LXC networking**: Check if unprivileged container has routing issues
5. **Kernel netfilter bug**: Check for known bugs in kernel 6.8.12-12-pve

## Recommendations

### Immediate Actions (Completed)
- ✅ Seafile accessible on port 8090
- ✅ NPM proxy updated
- ✅ HTTPS domain working
- ✅ All services healthy

### Short-term Monitoring
1. **Document port history**: Track when port 8090 might fail
2. **Monitor connection counts**: Track conntrack table usage
3. **Log network events**: Alert on network service restarts
4. **Test external access daily**: Automated curl checks from multiple hosts

### Long-term Investigation
1. **Install conntrack tool**: Monitor connection tracking state
2. **Check OPNsense firewall**: Review state table and NAT rules
3. **Review Proxmox networking**: Check LXC bridge configuration
4. **Test port lifecycle**: Systematically test port failure conditions
5. **Capture packet traces**: Use tcpdump during failure events

### Defensive Measures
1. **Port rotation schedule**: Change port before failure (monthly?)
2. **Multiple port mapping**: Expose 8090, 8091, 8092 simultaneously
3. **Health monitoring**: Alert when external access fails
4. **Automated remediation**: Script to change port and update NPM on failure

## Lessons Learned

### Key Insights
1. **"Crash" diagnosis was wrong**: Container never crashed, only port connectivity failed
2. **Always check container state**: Use `docker inspect` to verify actual crash
3. **Network != Application**: Port issues don't mean application failure
4. **Quick fix vs root cause**: Sometimes workaround is faster than deep diagnosis
5. **Document patterns**: This is the 3rd port issue, pattern is clear

### Troubleshooting Methodology
1. ✅ **Verify services running** before assuming crash
2. ✅ **Test from multiple hosts** to isolate network issues
3. ✅ **Check Docker state** to differentiate container vs port issues
4. ✅ **Port change as diagnostic** tool confirms port-specific problem
5. ✅ **Document everything** for pattern recognition

### What Worked
- **Port change workaround**: Immediate service restoration
- **Systematic testing**: Identified exact failure points
- **Multi-layer diagnosis**: Checked PCT, Docker, iptables, app layers

### What Didn't Work
- **Container restart**: Port issue persisted
- **PCT reboot**: Port issue persisted
- **Service-level fixes**: Not applicable (services weren't broken)

## Future Research Questions

1. Why does my machine (192.168.1.134) have working connectivity when others fail?
2. Is there a pattern to which ports fail (50080, 8080) vs work (8090, 3306)?
3. Does OPNsense (192.168.1.3 gateway) have stale firewall state?
4. Is Proxmox LXC bridge (vmbr0) corrupting port forwards?
5. Can we reproduce the issue in a test environment?

## Technical Appendix

### Environment Details
- **Proxmox Host**: pve2 (kernel 6.8.12-12-pve)
- **PCT Container**: 103 (Debian unprivileged, Docker enabled)
- **Docker Version**: (check with `docker --version`)
- **Network Bridge**: vmbr0
- **Gateway**: 192.168.1.3 (OPNsense)
- **DNS**: 192.168.1.5 (Pi-hole)

### Port History
```
Original (Sept 25):    50080 → Failed (Sept 27)
First fix (Sept 27):   8080  → Failed (Oct 7)
Second fix (Oct 7):    8090  → Currently working
```

### Related Documentation
- `/docs/seafile/seafile-infrastructure.md` - Infrastructure overview
- `/docs/seafile/seafile-crash-resolution-2025-09-27.md` - Sept 27 incident
- `/docs/seafile/seafile-network-fix-2025-09-27.md` - Sept 27 port 50080→8080 fix
- `/docs/seafile/troubleshooting-guide.md` - General troubleshooting

---

**Investigation completed**: October 7, 2025 22:10 GMT+2
**Resolution time**: 2 hours 10 minutes
**Service status**: Fully operational on port 8090
**Root cause**: Unknown port-specific connectivity failure (recurring pattern)
**Workaround**: Port change from 8080 to 8090
**Next occurrence**: Expected in weeks-months based on pattern

*Documented by: Claude Code Assistant*
*Incident classification: Port connectivity issue, NOT application crash*
